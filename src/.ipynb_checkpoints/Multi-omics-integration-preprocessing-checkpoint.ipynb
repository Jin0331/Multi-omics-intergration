{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER_TYPE = \"LIHC\"\n",
    "RAW_file_path = \"/home/km/gitworking/Multi-omics-intergration/RAW_DATA/\"\n",
    "PKL_file_path = \"/home/km/gitworking/Multi-omics-intergration/pkl/\"\n",
    "MODEL_path = \"/home/km/gitworking/Multi-omics-intergration/models/\"\n",
    "TENSORBOARD_PATH = '/home/km/gitworking/Multi-omics-intergration/log'\n",
    "GROUP_PHTH = '/home/km/gitworking/Multi-omics-intergration/group/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPU cehck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "tf.test.is_built_with_cuda()\n",
    "tf.config.list_physical_devices('GPU')\n",
    "tf.sysconfig.get_build_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3_unweighted\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# rpy2\n",
    "os.environ['R_HOME'] = '/home/km/anaconda3/envs/multiomics/lib/R' # env R invoke\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "#!python -m rpy2.situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancer_select(cols, cancer_type):\n",
    "    # phenotype\n",
    "    phe1 = pd.read_csv(\"https://gdc-hub.s3.us-east-1.amazonaws.com/download/GDC-PANCAN.basic_phenotype.tsv.gz\", sep=\"\\t\")\n",
    "    phe1 = phe1.loc[phe1.program == \"TCGA\", :].loc[:, ['sample', 'sample_type', 'project_id']].drop_duplicates(['sample'])\n",
    "    phe1['sample'] =  phe1.apply(lambda x : x['sample'][:-1], axis=1)\n",
    "    phe2 = pd.read_csv(\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/TCGA_phenotype_denseDataOnlyDownload.tsv.gz\", sep=\"\\t\")\n",
    "    ph_join = pd.merge(left = phe2 , right = phe1, how = \"left\", on = \"sample\").dropna(subset=['project_id'])\n",
    "    \n",
    "    if cancer_type == \"PAN\" or cancer_type == \"PANCAN\":\n",
    "        filterd = ph_join.loc[ph_join['sample_type_y'] == \"Primary Tumor\", :]\n",
    "        sample_barcode = filterd[\"sample\"].tolist()\n",
    "    else:\n",
    "        filterd = ph_join.loc[(ph_join['sample_type_y'] == \"Primary Tumor\") & (ph_join['project_id'] == \"TCGA-\" + cancer_type) , :]\n",
    "        sample_barcode = filterd[\"sample\"].tolist()\n",
    "        \n",
    "    intersect_ = list(set(cols).intersection(sample_barcode))\n",
    "    \n",
    "    return intersect_\n",
    "\n",
    "def non_zero_column(DF):\n",
    "    sample_cnt = int(len(DF.columns) * 0.2)\n",
    "    zero_row = dict(DF.isin([0]).sum(axis=1))\n",
    "    non_remove_feature = list()\n",
    "\n",
    "    for key, value in zero_row.items():\n",
    "        if value < sample_cnt:\n",
    "            non_remove_feature.append(key)\n",
    "    \n",
    "    return non_remove_feature\n",
    "\n",
    "def load_tcga_dataset(pkl_path, raw_path, cancer_type, norm):\n",
    "    \n",
    "    if os.path.isfile(pkl_path + \"/\" + cancer_type + \"_omics.pkl\"):\n",
    "        omics = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_omics.pkl\")\n",
    "\n",
    "        # sep\n",
    "        rna = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_rna.pkl\")\n",
    "        mirna = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_mirna.pkl\")\n",
    "        mt = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_mt.pkl\")\n",
    "        \n",
    "        # intersect\n",
    "        venn3_unweighted([set(rna.index), set(mirna.index), set(mt.index)], ('RNA', 'miRNA', 'Methylation'))\n",
    "        plt.show()\n",
    "        \n",
    "    else :\n",
    "        # RNA gene expression\n",
    "        col = pd.read_csv(raw_path + \"tcga_RSEM_Hugo_norm_count.gz\",\n",
    "                     sep = \"\\t\", index_col=0, nrows=0).columns.to_list()\n",
    "        use_col = ['sample'] + cancer_select(cols=col, cancer_type=cancer_type)\n",
    "        df_chunk = pd.read_csv(raw_path + \"tcga_RSEM_Hugo_norm_count.gz\",\n",
    "                     sep = \"\\t\", index_col=0, iterator=True, chunksize=50000, usecols=use_col)\n",
    "        rna = pd.concat([chunk for chunk in df_chunk])\n",
    "        rna = rna[rna.index.isin(non_zero_column(rna))].T\n",
    "        \n",
    "        rna.to_pickle(pkl_path + \"/\" + cancer_type + \"_rna.pkl\")\n",
    "\n",
    "        # miRNA expression\n",
    "        col = pd.read_csv(raw_path + \"pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena.gz\",\n",
    "                     sep = \"\\t\", index_col=0, nrows=0).columns.to_list()\n",
    "        use_col = ['sample'] + cancer_select(cols=col, cancer_type=cancer_type)\n",
    "\n",
    "        df_chunk = pd.read_csv(raw_path + \"pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena.gz\",\n",
    "                         sep = \"\\t\", index_col=0, iterator=True, chunksize=50000, usecols=use_col)\n",
    "        mirna = pd.concat([chunk for chunk in df_chunk])\n",
    "        mirna = mirna[mirna.index.isin(non_zero_column(mirna))].T\n",
    "        \n",
    "        mirna.to_pickle(pkl_path + \"/\" + cancer_type + \"_mirna.pkl\")\n",
    "\n",
    "        # methylation\n",
    "        col = pd.read_csv(raw_path + \"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena.gz\",\n",
    "                     sep = \"\\t\", index_col=0, nrows=0).columns.to_list()\n",
    "        use_col = ['sample'] + cancer_select(cols=col, cancer_type=cancer_type)\n",
    "\n",
    "        df_chunk = pd.read_csv(raw_path + \"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena.gz\",\n",
    "                         sep = \"\\t\", index_col=0, iterator=True, chunksize=50000, usecols=use_col)\n",
    "        mt = pd.concat([chunk for chunk in df_chunk])\n",
    "\n",
    "        mt_map = pd.read_csv(raw_path + \"probeMap_illuminaMethyl450_hg19_GPL16304_TCGAlegacy\", sep=\"\\t\")\n",
    "\n",
    "        mt_join = pd.merge(mt, mt_map, how = \"left\", left_on = \"sample\", right_on = \"#id\")\\\n",
    "                 .drop(['chrom', 'chromStart', 'chromEnd', 'strand', '#id'], axis=1)\n",
    "        mt_join = mt_join[mt_join.gene != \".\"]\n",
    "        mt_join.dropna(subset = [\"gene\"], inplace=True)\n",
    "\n",
    "        # gene mean \n",
    "        mt_join_gene_filter = mt_join.groupby(['gene']).mean()\n",
    "        mt_join_gene_filter = mt_join_gene_filter[mt_join_gene_filter.index.isin(non_zero_column(mt_join_gene_filter))].T\n",
    "        \n",
    "        mt_join_gene_filter.to_pickle(pkl_path + \"/\" + cancer_type + \"_mt.pkl\")\n",
    "        \n",
    "        # intersect\n",
    "        venn3_unweighted([set(rna.index), set(mirna.index), set(mt_join_gene_filter.index)], ('RNA', 'miRNA', 'Methylation'))\n",
    "        plt.show()\n",
    "        \n",
    "        # set same column for merge\n",
    "        rna['sample'] = rna.index\n",
    "        mirna['sample'] = mirna.index\n",
    "        mt_join_gene_filter['sample'] = mt_join_gene_filter.index\n",
    "\n",
    "        # data join\n",
    "        merge_list = [rna, mirna, mt_join_gene_filter]\n",
    "        omics = reduce(lambda left, right : pd.merge(left, right, on = \"sample\"), merge_list)\n",
    "        omics.set_index('sample', inplace=True)\n",
    "\n",
    "        # pickle save\n",
    "        omics.to_pickle(pkl_path + \"/\" + cancer_type + \"_omics.pkl\")\n",
    "    \n",
    "    # set index\n",
    "    omics_index = omics.index.to_list()\n",
    "    \n",
    "    # normalization\n",
    "    if norm:\n",
    "        scalerX = StandardScaler()\n",
    "        omics_scale = scalerX.fit_transform(omics)\n",
    "    \n",
    "    # missing impute\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    omics_impute = imputer.fit_transform(omics_scale)\n",
    "\n",
    "    omics = pd.DataFrame(omics_impute, columns=omics.columns)\n",
    "    omics.index = omics_index\n",
    "\n",
    "    return omics\n",
    "\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "    return K.sqrt(msle(y_true, y_pred)) \n",
    "\n",
    "def make_Tensorboard_dir(dir_name):\n",
    "    root_logdir = os.path.join(os.curdir, dir_name) \n",
    "    sub_dir_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "    return os.path.join(root_logdir, sub_dir_name)\n",
    "\n",
    "\n",
    "def createFolder(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print ('Folder already exists. ' +  path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data-Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = load_tcga_dataset(pkl_path=PKL_file_path, raw_path=RAW_file_path, cancer_type=CANCER_TYPE, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(omics, test_size = .2, random_state = 21, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Auto-Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **end-to-end**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs_dim = X_train.shape[1]\n",
    "encoder = Input(shape = (inputs_dim, ))\n",
    "e = Dense(1000, activation = \"relu\")(encoder)\n",
    "# e = Dense(256, activation = \"relu\")(e)\n",
    "e = Dense(500, activation = \"relu\")(e)\n",
    "\n",
    "## bottleneck layer\n",
    "n_bottleneck = 100\n",
    "\n",
    "## defining it with a name to extract it later\n",
    "bottleneck_layer = \"bottleneck_layer\"\n",
    "\n",
    "# can also be defined with an activation function, relu for instance\n",
    "bottleneck = Dense(n_bottleneck, name = bottleneck_layer)(e)\n",
    "\n",
    "## define the decoder (in reverse)\n",
    "decoder = Dense(500, activation = \"relu\")(bottleneck)\n",
    "# decoder = Dense(256, activation = \"relu\")(decoder)\n",
    "decoder = Dense(1000, activation = \"relu\")(decoder)\n",
    "\n",
    "## output layer\n",
    "output = Dense(inputs_dim)(decoder)\n",
    "\n",
    "## model\n",
    "model = Model(inputs = encoder, outputs = output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Encoder for Latent Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs = model.input, outputs = bottleneck)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Model compile & Fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback function\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "TB_log_dir = make_Tensorboard_dir(TENSORBOARD_PATH)\n",
    "TensorB = tf.keras.callbacks.TensorBoard(log_dir = TB_log_dir)\n",
    "\n",
    "# compile & fit\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "              optimizer = \"adam\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    X_train,\n",
    "    batch_size = 128,\n",
    "    epochs = 30,\n",
    "    verbose = 0,\n",
    "    validation_data = (X_test, X_test),\n",
    "    callbacks=[TensorB]\n",
    ")\n",
    "file_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "model.save(MODEL_path + \"AE_\" + CANCER_TYPE + \"_\" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "omic_encoded = pd.DataFrame(encoder.predict(omics))\n",
    "column_name = [\"Feature\" + str(index) for index in range(1, len(omic_encoded.columns) + 1)]\n",
    "omic_encoded.columns = column_name\n",
    "\n",
    "omic_encoded['sample'] = omics.index.to_list()\n",
    "omic_encoded.set_index('sample', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **sample phenotype**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno = pd.read_csv(\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/Survival_SupplementalTable_S1_20171025_xena_sp\", \n",
    "                    sep = \"\\t\", usecols=['sample', 'OS', 'OS.time', 'DSS', 'DSS.time', 'DFI', 'DFI.time', 'PFI', 'PFI.time'])\n",
    "\n",
    "# pd.merge(left, right, how, on, left_on, right_on, left_index, right_index)\n",
    "omic_encoded_pheno = pd.merge(left=omic_encoded, right=pheno, how=\"inner\", on=\"sample\")\n",
    "omic_encoded_pheno.set_index('sample', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Invoke R - log rank test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas DF to R DF\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    r_from_pd_df = ro.conversion.py2rpy(omic_encoded_pheno)\n",
    "\n",
    "# R UDF invoke\n",
    "r = ro.r\n",
    "r['source']('r-function.R')\n",
    "log_rank_test_r = ro.globalenv['log_rank_test']\n",
    "log_rank_test_feature = log_rank_test_r(r_from_pd_df)\n",
    "\n",
    "# R DF to pandas DF\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    log_rank_test = ro.conversion.rpy2py(log_rank_test_feature)\n",
    "\n",
    "feature_log = log_rank_test['Features'].to_list()\n",
    "\n",
    "omic_encoded_fc = omic_encoded[feature_log]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sample Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas DF to R DF\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    omic_encoded_fc_r = ro.conversion.py2rpy(omic_encoded_fc)\n",
    "\n",
    "r = ro.r\n",
    "r['source']('r-function.R')\n",
    "nb_cluster_test = ro.globalenv['nb_cluster_test']\n",
    "nb_cluster_test_feature = nb_cluster_test(omic_encoded_fc_r)\n",
    "\n",
    "# R DF to pandas DF\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    omic_encoded_fc_r = ro.conversion.rpy2py(nb_cluster_test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **K-Mean Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2, random_state = 31, max_iter = 1000).fit_predict(omic_encoded_fc)\n",
    "ae_groups = pd.DataFrame(kmeans, columns = ['group'])\n",
    "ae_groups['sample'] = omic_encoded_fc.index.to_list()\n",
    "ae_groups.set_index('sample', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "ae_groups.to_csv(GROUP_PHTH + CANCER_TYPE + \"_GROUP_\" + time_stamp + \".txt\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomic",
   "language": "python",
   "name": "multiomics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
