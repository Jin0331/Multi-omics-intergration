{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70481502-18c4-440f-ac31-b934b0232ed5",
   "metadata": {},
   "source": [
    "### **Tensorflow-GPU cehck**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677597a0-e603-4986-8da5-13811d50a0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# GPU memory growth true\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7506f-5d91-4dbb-a14a-89ffb9796430",
   "metadata": {},
   "source": [
    "## **Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b07b9-1261-4670-8ed9-87036110db6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3_unweighted\n",
    "from matplotlib import rcParams\n",
    "from IPython.display import SVG\n",
    "from numba import cuda\n",
    "\n",
    "# lifelines\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import k_fold_cross_validation\n",
    "\n",
    "# sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Add, Multiply, Layer, BatchNormalization, Activation\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,Callback\n",
    "from tensorflow.keras import metrics, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "\n",
    "# rpy2\n",
    "os.environ['R_HOME'] = '/home/km/anaconda3/envs/multiomics/lib/R' # env R invoke\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cd486-13bb-458f-9ece-8d6087afeb95",
   "metadata": {},
   "source": [
    "## **UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d84eb-e0a4-43af-8bfd-5a2ff522e6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cancer_select(cols, cancer_type):\n",
    "    # phenotype\n",
    "    phe1 = pd.read_csv(\"https://gdc-hub.s3.us-east-1.amazonaws.com/download/GDC-PANCAN.basic_phenotype.tsv.gz\", sep=\"\\t\")\n",
    "    phe1 = phe1.loc[phe1.program == \"TCGA\", :].loc[:, ['sample', 'sample_type', 'project_id']].drop_duplicates(['sample'])\n",
    "    phe1['sample'] =  phe1.apply(lambda x : x['sample'][:-1], axis=1)\n",
    "    phe2 = pd.read_csv(\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/TCGA_phenotype_denseDataOnlyDownload.tsv.gz\", sep=\"\\t\")\n",
    "    ph_join = pd.merge(left = phe2 , right = phe1, how = \"left\", on = \"sample\").dropna(subset=['project_id'])\n",
    "    \n",
    "    if cancer_type == \"PAN\" or cancer_type == \"PANCAN\":\n",
    "        filterd = ph_join.loc[ph_join['sample_type_y'] == \"Primary Tumor\", :]\n",
    "        sample_barcode = filterd[\"sample\"].tolist()\n",
    "    else:\n",
    "        filterd = ph_join.loc[(ph_join['sample_type_y'] == \"Primary Tumor\") & (ph_join['project_id'] == \"TCGA-\" + cancer_type) , :]\n",
    "        sample_barcode = filterd[\"sample\"].tolist()\n",
    "        \n",
    "    intersect_ = list(set(cols).intersection(sample_barcode))\n",
    "    \n",
    "    return intersect_\n",
    "\n",
    "def non_zero_column(DF):\n",
    "    sample_cnt = int(len(DF.columns) * 0.2)\n",
    "    zero_row = dict(DF.isin([0]).sum(axis=1))\n",
    "    non_remove_feature = list()\n",
    "\n",
    "    for key, value in zero_row.items():\n",
    "        if value < sample_cnt:\n",
    "            non_remove_feature.append(key)\n",
    "    \n",
    "    return non_remove_feature\n",
    "\n",
    "def load_tcga_dataset(pkl_path, raw_path, cancer_type, norm, minmax=None):\n",
    "    \n",
    "    if os.path.isfile(pkl_path + \"/\" + cancer_type + \"_omics.pkl\"):\n",
    "        omics = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_omics.pkl\")\n",
    "\n",
    "        # sep\n",
    "        rna = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_rna.pkl\")\n",
    "        mirna = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_mirna.pkl\")\n",
    "        mt = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_mt.pkl\")\n",
    "        \n",
    "        # intersect\n",
    "        venn3_unweighted([set(rna.index), set(mirna.index), set(mt.index)], ('RNA', 'miRNA', 'Methylation'))\n",
    "        plt.show()\n",
    "        \n",
    "    else :\n",
    "        # RNA gene expression\n",
    "        col = pd.read_csv(raw_path + \"tcga_RSEM_Hugo_norm_count.gz\",\n",
    "                     sep = \"\\t\", index_col=0, nrows=0).columns.to_list()\n",
    "        use_col = ['sample'] + cancer_select(cols=col, cancer_type=cancer_type)\n",
    "        df_chunk = pd.read_csv(raw_path + \"tcga_RSEM_Hugo_norm_count.gz\",\n",
    "                     sep = \"\\t\", index_col=0, iterator=True, chunksize=50000, usecols=use_col)\n",
    "        rna = pd.concat([chunk for chunk in df_chunk])\n",
    "        rna = rna[rna.index.isin(non_zero_column(rna))].T\n",
    "        \n",
    "        rna.to_pickle(pkl_path + \"/\" + cancer_type + \"_rna.pkl\")\n",
    "\n",
    "        # miRNA expression\n",
    "        col = pd.read_csv(raw_path + \"pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena.gz\",\n",
    "                     sep = \"\\t\", index_col=0, nrows=0).columns.to_list()\n",
    "        use_col = ['sample'] + cancer_select(cols=col, cancer_type=cancer_type)\n",
    "\n",
    "        df_chunk = pd.read_csv(raw_path + \"pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena.gz\",\n",
    "                         sep = \"\\t\", index_col=0, iterator=True, chunksize=50000, usecols=use_col)\n",
    "        mirna = pd.concat([chunk for chunk in df_chunk])\n",
    "        mirna = mirna[mirna.index.isin(non_zero_column(mirna))].T\n",
    "        \n",
    "        mirna.to_pickle(pkl_path + \"/\" + cancer_type + \"_mirna.pkl\")\n",
    "\n",
    "        # methylation\n",
    "        col = pd.read_csv(raw_path + \"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena.gz\",\n",
    "                     sep = \"\\t\", index_col=0, nrows=0).columns.to_list()\n",
    "        use_col = ['sample'] + cancer_select(cols=col, cancer_type=cancer_type)\n",
    "\n",
    "        df_chunk = pd.read_csv(raw_path + \"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena.gz\",\n",
    "                         sep = \"\\t\", index_col=0, iterator=True, chunksize=50000, usecols=use_col)\n",
    "        mt = pd.concat([chunk for chunk in df_chunk])\n",
    "\n",
    "        mt_map = pd.read_csv(raw_path + \"probeMap_illuminaMethyl450_hg19_GPL16304_TCGAlegacy\", sep=\"\\t\")\n",
    "\n",
    "        mt_join = pd.merge(mt, mt_map, how = \"left\", left_on = \"sample\", right_on = \"#id\")\\\n",
    "                 .drop(['chrom', 'chromStart', 'chromEnd', 'strand', '#id'], axis=1)\n",
    "        mt_join = mt_join[mt_join.gene != \".\"]\n",
    "        mt_join.dropna(subset = [\"gene\"], inplace=True)\n",
    "\n",
    "        # gene mean \n",
    "        mt_join_gene_filter = mt_join.groupby(['gene']).mean()\n",
    "        mt_join_gene_filter = mt_join_gene_filter[mt_join_gene_filter.index.isin(non_zero_column(mt_join_gene_filter))].T\n",
    "        \n",
    "        mt_join_gene_filter.to_pickle(pkl_path + \"/\" + cancer_type + \"_mt.pkl\")\n",
    "        \n",
    "        # intersect\n",
    "        venn3_unweighted([set(rna.index), set(mirna.index), set(mt_join_gene_filter.index)], ('RNA', 'miRNA', 'Methylation'))\n",
    "        plt.show()\n",
    "        \n",
    "        # set same column for merge\n",
    "        rna['sample'] = rna.index\n",
    "        mirna['sample'] = mirna.index\n",
    "        mt_join_gene_filter['sample'] = mt_join_gene_filter.index\n",
    "\n",
    "        # data join\n",
    "        merge_list = [rna, mirna, mt_join_gene_filter]\n",
    "        omics = reduce(lambda left, right : pd.merge(left, right, on = \"sample\"), merge_list)\n",
    "        omics.set_index('sample', inplace=True)\n",
    "\n",
    "        # pickle save\n",
    "        omics.to_pickle(pkl_path + \"/\" + cancer_type + \"_omics.pkl\")\n",
    "    \n",
    "    # set index\n",
    "    omics_index = omics.index.to_list()\n",
    "    \n",
    "    # normalization\n",
    "    if norm:\n",
    "        if minmax:\n",
    "            scalerX = MinMaxScaler()\n",
    "            omics_scale = scalerX.fit_transform(omics)\n",
    "        else :\n",
    "            scalerX = StandardScaler()      \n",
    "            omics_scale = scalerX.fit_transform(omics)\n",
    "    \n",
    "    # missing impute\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    omics_impute = imputer.fit_transform(omics_scale)\n",
    "\n",
    "    omics = pd.DataFrame(omics_impute, columns=omics.columns)\n",
    "    omics.index = omics_index\n",
    "\n",
    "    return omics\n",
    "\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "    return K.sqrt(msle(y_true, y_pred)) \n",
    "\n",
    "def make_Tensorboard_dir(dir_name):\n",
    "    root_logdir = os.path.join(os.curdir, dir_name) \n",
    "    sub_dir_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "    return os.path.join(root_logdir, sub_dir_name)\n",
    "\n",
    "# autoencoder\n",
    "def run_ae(X_train, X_test, tensorboard_path):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # encoder - decoder\n",
    "    inputs_dim = X_train.shape[1]\n",
    "    encoder = Input(shape = (inputs_dim, ))\n",
    "    e = Dense(1000, activation = \"tanh\")(encoder)\n",
    "    e = Dense(500, activation = \"tanh\")(e)\n",
    "\n",
    "    ## bottleneck layer\n",
    "    n_bottleneck = 100\n",
    "\n",
    "    ## defining it with a name to extract it later\n",
    "    bottleneck_layer = \"bottleneck_layer\"\n",
    "\n",
    "    # can also be defined with an activation function, relu for instance\n",
    "    bottleneck = Dense(n_bottleneck, name = bottleneck_layer)(e)\n",
    "\n",
    "    ## define the decoder (in reverse)\n",
    "    decoder = Dense(500, activation = \"tanh\")(bottleneck)\n",
    "    # decoder = Dense(256, activation = \"relu\")(decoder)\n",
    "    decoder = Dense(1000, activation = \"tanh\")(decoder)\n",
    "\n",
    "    ## output layer\n",
    "    output = Dense(inputs_dim)(decoder)\n",
    "\n",
    "    ## end-to-end model\n",
    "    model = Model(inputs = encoder, outputs = output)\n",
    "\n",
    "    # encdoer mdoel\n",
    "    encoder = Model(inputs = model.input, outputs = bottleneck)\n",
    "\n",
    "    # callback function\n",
    "    # es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    TB_log_dir = make_Tensorboard_dir(tensorboard_path)\n",
    "    TensorB = tf.keras.callbacks.TensorBoard(log_dir = TB_log_dir)\n",
    "\n",
    "    # compile & fit\n",
    "    model.compile(loss = \"mean_squared_error\",\n",
    "                  optimizer = \"adam\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        X_train,\n",
    "        batch_size = 128,\n",
    "        epochs = 30,\n",
    "        verbose = 0,\n",
    "        validation_data = (X_test, X_test),\n",
    "        callbacks = [TensorB]\n",
    "    ) \n",
    "    \n",
    "    return encoder\n",
    "\n",
    "# autoencoder\n",
    "def run_ae_denoisy(X_train, X_test, tensorboard_path):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # noise\n",
    "    noise_factor = 0.5\n",
    "    X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=0.3, size=X_train.shape)\n",
    "    X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=0.3, size=X_test.shape)\n",
    "\n",
    "    # encoder - decoder\n",
    "    inputs_dim = X_train.shape[1]\n",
    "    encoder = Input(shape = (inputs_dim, ))\n",
    "    e = Dense(1000, activation = \"tanh\")(encoder)\n",
    "    e = Dense(500, activation = \"tanh\")(e)\n",
    "\n",
    "    ## bottleneck layer\n",
    "    n_bottleneck = 100\n",
    "\n",
    "    ## defining it with a name to extract it later\n",
    "    bottleneck_layer = \"bottleneck_layer\"\n",
    "\n",
    "    # can also be defined with an activation function, relu for instance\n",
    "    bottleneck = Dense(n_bottleneck, name = bottleneck_layer)(e)\n",
    "\n",
    "    ## define the decoder (in reverse)\n",
    "    decoder = Dense(500, activation = \"tanh\")(bottleneck)\n",
    "    # decoder = Dense(256, activation = \"relu\")(decoder)\n",
    "    decoder = Dense(1000, activation = \"tanh\")(decoder)\n",
    "\n",
    "    ## output layer\n",
    "    output = Dense(inputs_dim)(decoder)\n",
    "\n",
    "    ## end-to-end model\n",
    "    model = Model(inputs = encoder, outputs = output)\n",
    "\n",
    "    # encdoer mdoel\n",
    "    encoder = Model(inputs = model.input, outputs = bottleneck)\n",
    "\n",
    "    # callback function\n",
    "    # es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    TB_log_dir = make_Tensorboard_dir(tensorboard_path)\n",
    "    TensorB = tf.keras.callbacks.TensorBoard(log_dir = TB_log_dir)\n",
    "\n",
    "    # compile & fit\n",
    "    model.compile(loss = \"mean_squared_error\",\n",
    "                  optimizer = \"adam\")\n",
    "    history = model.fit(\n",
    "        X_train_noisy,\n",
    "        X_train_noisy,\n",
    "        batch_size = 128,\n",
    "        epochs = 30,\n",
    "        verbose = 0,\n",
    "        validation_data = (X_test, X_test),\n",
    "        callbacks = [TensorB]\n",
    "    ) \n",
    "    \n",
    "    return encoder\n",
    "\n",
    "def run_ae_sparse(X_train, X_test, tensorboard_path):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # encoder - decoder\n",
    "    inputs_dim = X_train.shape[1]\n",
    "    encoder = Input(shape = (inputs_dim, ))\n",
    "    e = Dense(1000, activation = 'tanh',activity_regularizer=tf.keras.regularizers.l1_l2(l1=0.000001,l2=0.00001))(encoder)\n",
    "    e = Dense(1000, activation = \"tanh\")(e)\n",
    "\n",
    "    ## define the decoder (in reverse)\n",
    "    decoder = Dense(1000, activation = 'tanh',activity_regularizer=tf.keras.regularizers.l1_l2(l1=0.000001,l2=0.00001))(e)\n",
    "    output = Dense(inputs_dim)(decoder)\n",
    "\n",
    "    ## end-to-end model\n",
    "    model = Model(inputs = encoder, outputs = output)\n",
    "\n",
    "    # encdoer mdoel\n",
    "    encoder = Model(inputs = model.input, outputs = e)\n",
    "\n",
    "    # callback function\n",
    "    # es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    TB_log_dir = make_Tensorboard_dir(tensorboard_path)\n",
    "    TensorB = tf.keras.callbacks.TensorBoard(log_dir = TB_log_dir)\n",
    "\n",
    "    # compile & fit\n",
    "    model.compile(loss = \"mean_squared_error\",\n",
    "                  optimizer = \"adam\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        X_train,\n",
    "        batch_size = 128,\n",
    "        epochs = 30,\n",
    "        verbose = 0,\n",
    "        validation_data = (X_test, X_test),\n",
    "        callbacks = [TensorB]\n",
    "    ) \n",
    "    \n",
    "    return encoder\n",
    "\n",
    "# invoke r\n",
    "def log_test(df):\n",
    "    # pandas DF to R DF\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        r_from_pd_df = ro.conversion.py2rpy(df)\n",
    "\n",
    "    # R UDF invoke\n",
    "    r = ro.r\n",
    "    r['source']('src/r-function.R')\n",
    "    log_rank_test_r = ro.globalenv['log_rank_test']\n",
    "    log_rank_test_feature = log_rank_test_r(r_from_pd_df)\n",
    "\n",
    "    # R DF to pandas DF\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        log_rank_test = ro.conversion.rpy2py(log_rank_test_feature)\n",
    "\n",
    "    feature_log = log_rank_test['Features'].to_list()\n",
    "    \n",
    "    return feature_log\n",
    "\n",
    "def log_rank_test(df, png_path, file_name):\n",
    "    group_size = len(set(df.loc[:, [\"group\"]].iloc[:,0].to_list()))\n",
    "    labels = [\"G\" + str(index) for index in range(group_size)]\n",
    "    \n",
    "    groups = df.dropna().iloc[:, 0].to_list()\n",
    "    events = df.dropna().iloc[:, 1].to_list()\n",
    "    times = df.dropna().iloc[:, 2].to_list()\n",
    "\n",
    "    E = np.array(events, dtype=np.int32)\n",
    "    T = np.array(times, dtype=np.float32)\n",
    "    \n",
    "    #### 3. matplotlib parameter\n",
    "    rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    styles = ['-', '--']\n",
    "    colors = ['r', 'g']\n",
    "    lw = 3\n",
    "\n",
    "    #### 4. Kaplan-Meier \n",
    "    kmf = KaplanMeierFitter()\n",
    "    for i, label in enumerate(labels):\n",
    "        ix = np.array(groups) == i\n",
    "        kmf.fit(T[ix], event_observed=E[ix], label=labels[i])\n",
    "        kmf.plot(ax=ax, ci_show=False, linewidth=lw, style=styles[i], c=colors[i])\n",
    "\n",
    "    #### 5. Logrank test\n",
    "    ix = np.array(groups) == 1\n",
    "    result = logrank_test(T[ix], T[~ix], E[ix], E[~ix], alpha=.99)\n",
    "    pvalue = result.p_value\n",
    "    ax.text(50,0.3,'P-value=%.6f'% pvalue) # 위치(3.4,0.75) 수동으로 지정필요\n",
    "\n",
    "    #### 6. \n",
    "    ax.set_xlabel('Time', fontsize=14)\n",
    "    ax.set_ylabel('Survival', fontsize=14)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path + file_name + \"_logrank.png\", format='png', dpi=300)\n",
    "\n",
    "    return pvalue\n",
    "\n",
    "# invoke r\n",
    "def nb_cluster(df):\n",
    "    # pandas DF to R DF\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        omic_encoded_fc_r = ro.conversion.py2rpy(df)\n",
    "\n",
    "    r = ro.r\n",
    "    r['source']('src/r-function.R')\n",
    "    nb_cluster_test = ro.globalenv['nb_cluster_test']\n",
    "    nb_cluster_test_feature = nb_cluster_test(omic_encoded_fc_r)\n",
    "\n",
    "    # R DF to pandas DF\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        omic_encoded_fc_r = ro.conversion.rpy2py(nb_cluster_test_feature)\n",
    "        \n",
    "    return omic_encoded_fc_r\n",
    "\n",
    "# invoke r\n",
    "def survfit(df, file_name, PNG_PATH):\n",
    "    # pandas DF to R DF\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        r_from_pd_df = ro.conversion.py2rpy(df)\n",
    "\n",
    "    # R UDF invoke\n",
    "    r = ro.r\n",
    "    r['source']('src/r-function.R')\n",
    "    km_survival_r = ro.globalenv['km_survival']\n",
    "    km_survival_r(r_from_pd_df, file_name, PNG_PATH)\n",
    "\n",
    "def best_ae_model(model_list, o, group_path, model_path, cancer_type, file_name, \n",
    "                  model_names = ['encoder_vanilla', 'encoder_sparse', 'encoder_denoisy']):\n",
    "    \n",
    "    def model_prediction(model):\n",
    "        \n",
    "        if model[1] == 'encoder_denoisy':\n",
    "            noise_factor = 0.5\n",
    "            o2 = o + noise_factor + np.random.normal(loc=0.0, scale=0.3, size=o.shape)\n",
    "        else :\n",
    "            o2 = o\n",
    "        \n",
    "        omic_encoded = pd.DataFrame(model[0].predict(o2))\n",
    "        column_name = [\"Feature\" + str(index) for index in range(1, len(omic_encoded.columns) + 1)]\n",
    "        omic_encoded.columns = column_name\n",
    "\n",
    "        omic_encoded['sample'] = o2.index.to_list()\n",
    "        omic_encoded.set_index('sample', inplace=True)\n",
    "        \n",
    "        pheno = pd.read_csv(\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/Survival_SupplementalTable_S1_20171025_xena_sp\", \n",
    "                    sep = \"\\t\", usecols=['sample', 'OS', 'OS.time', 'DSS', 'DSS.time', 'DFI', 'DFI.time', 'PFI', 'PFI.time'])\n",
    "\n",
    "        # encoded pheno\n",
    "        omic_encoded_pheno = pd.merge(left=omic_encoded, right=pheno, how=\"inner\", on=\"sample\")\n",
    "        omic_encoded_pheno.set_index('sample', inplace=True)\n",
    "        \n",
    "        # log rank test\n",
    "        omic_encoded_fc = omic_encoded[log_test(omic_encoded_pheno)]\n",
    "        nb_result = nb_cluster(omic_encoded_fc)\n",
    "        \n",
    "        return nb_result.iloc[:, 0].to_list()[0], omic_encoded_fc\n",
    "    \n",
    "    # each model, prediction\n",
    "    # nb_result_list[?][0] - silhoaute score\n",
    "    # nb_result_list[?][1] - encoded\n",
    "    \n",
    "    # best model selection\n",
    "    nb_result_list = list(map(model_prediction, list(zip(model_list, model_names))))\n",
    "    zipbObj = zip(model_names, list(zip(nb_result_list, model_list)))\n",
    "    model_sil = dict(zipbObj)\n",
    "    model_sil_sort = sorted(model_sil.items(), key = lambda item : item[1][0], reverse=True) \n",
    "    best_model_n, best_model, s_score, encoded = model_sil_sort[0][0], model_sil_sort[0][1][1], model_sil_sort[0][1][0][0], model_sil_sort[0][1][0][1]\n",
    "    \n",
    "    # model save\n",
    "    best_model.save(model_path + \"AE_\" + best_model_n + \"_\"+ cancer_type + \"_\" + file_name)\n",
    "    \n",
    "    pr = \"Best AE : {0}\\nSilhouette score : {1}\".format(best_model_n, s_score)\n",
    "    print(pr)\n",
    "      \n",
    "    \n",
    "    # k-mean clustering\n",
    "    clusterer = KMeans(n_clusters = 2, random_state = 31, max_iter = 1000)\n",
    "    kmeans = clusterer.fit_predict(encoded)\n",
    "    \n",
    "    ae_groups = pd.DataFrame(kmeans, columns = ['group'])\n",
    "    ae_groups['sample'] = encoded.index.to_list()\n",
    "    ae_groups.set_index('sample', inplace=True)\n",
    "    ae_groups.to_csv(GROUP_PHTH + CANCER_TYPE + \"_GROUP_\" + FILE_NAME + \".txt\", sep=\"\\t\")\n",
    "    \n",
    "    return ae_groups, s_score\n",
    "        \n",
    "    \n",
    "## For Target\n",
    "def load_preprocess_tcga_dataset(pkl_path, raw_path, group, norm, cancer_type):\n",
    "    \n",
    "    if os.path.isfile(pkl_path + \"/\" + cancer_type + \"_omics.pkl\"):\n",
    "        # sep\n",
    "#         omics = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_omics.pkl\")\n",
    "        rna = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_rna.pkl\")\n",
    "        mirna = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_mirna.pkl\")\n",
    "        mt_join_gene_filter = pd.read_pickle(pkl_path + \"/\" + cancer_type + \"_mt.pkl\")\n",
    "        \n",
    "    else :\n",
    "        raise Exception(\"omics's pkl not exist!\")\n",
    "        \n",
    "    # set column for unique\n",
    "#     omics.columns = list(map(lambda col : col + \"_OMICS\", omics.columns.to_list()))\n",
    "    rna.columns = list(map(lambda col : col + \"_RNA\", rna.columns.to_list()))\n",
    "    mirna.columns = list(map(lambda col : col + \"_miRNA\", mirna.columns.to_list()))\n",
    "    mt_join_gene_filter.columns = list(map(lambda col : col + \"_Methylation\", mt_join_gene_filter.columns.to_list()))\n",
    "        \n",
    "    # set index\n",
    "#     omics_index = omics.index.to_list()\n",
    "    rna_index = rna.index.to_list()\n",
    "    mirna_index = mirna.index.to_list()\n",
    "    mt_join_gene_filter_index = mt_join_gene_filter.index.to_list()\n",
    "    \n",
    "    # normalization\n",
    "    if norm:\n",
    "        scalerX = StandardScaler()\n",
    "#         omics_scale = scalerX.fit_transform(omics)\n",
    "        rna_scale = scalerX.fit_transform(rna)\n",
    "        mirna_scale = scalerX.fit_transform(mirna)\n",
    "        mt_join_gene_filter_scale = scalerX.fit_transform(mt_join_gene_filter)\n",
    "\n",
    "        # missing impute\n",
    "        imputer = KNNImputer(n_neighbors=10)\n",
    "#         omics_impute = imputer.fit_transform(omics_scale)        \n",
    "        rna_impute = imputer.fit_transform(rna_scale)\n",
    "        mirna_impute = imputer.fit_transform(mirna_scale)\n",
    "        mt_join_gene_filter_impute = imputer.fit_transform(mt_join_gene_filter_scale)\n",
    "\n",
    "        # Pandas\n",
    "#         omics = pd.DataFrame(omics_impute, columns=omics.columns)\n",
    "#         omics.index = omics_index       \n",
    "        \n",
    "        rna = pd.DataFrame(rna_impute, columns=rna.columns)\n",
    "        rna.index = rna_index\n",
    "\n",
    "        mirna = pd.DataFrame(mirna_impute, columns=mirna.columns)\n",
    "        mirna.index = mirna_index\n",
    "\n",
    "        mt = pd.DataFrame(mt_join_gene_filter_impute, columns=mt_join_gene_filter.columns)\n",
    "        mt.index = mt_join_gene_filter_index\n",
    "        \n",
    "    else :\n",
    "        # missing impute\n",
    "        imputer = KNNImputer(n_neighbors=10)\n",
    "#         omics_impute = imputer.fit_transform(omics)                \n",
    "        rna_impute = imputer.fit_transform(rna)\n",
    "        mirna_impute = imputer.fit_transform(mirna)\n",
    "        mt_join_gene_filter_impute = imputer.fit_transform(mt_join_gene_filtere)\n",
    "\n",
    "        # Pandas\n",
    "#         omics = pd.DataFrame(omics_impute, columns=omics.columns)\n",
    "#         omics.index = rna_index       \n",
    "        \n",
    "        rna = pd.DataFrame(rna_impute, columns=rna.columns)\n",
    "        rna.index = rna_index\n",
    "\n",
    "        mirna = pd.DataFrame(mirna_impute, columns=mirna.columns)\n",
    "        mirna.index = mirna_index\n",
    "\n",
    "        mt = pd.DataFrame(mt_join_gene_filter_impute, columns=mt_join_gene_filter.columns)\n",
    "        mt.index = mt_join_gene_filter_index\n",
    "    \n",
    "    # phenotype only omics \n",
    "    pheno = pd.read_csv(\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/Survival_SupplementalTable_S1_20171025_xena_sp\", \n",
    "                    sep = \"\\t\", usecols=['sample', 'OS', 'OS.time', 'DSS', 'DSS.time', 'DFI', 'DFI.time', 'PFI', 'PFI.time'])\n",
    "    pheno.set_index('sample', inplace=True)\n",
    "    \n",
    "    join_list = [rna, mirna, mt]\n",
    "    omics = reduce(lambda left, right : pd.merge(left = left, right = right, how = \"inner\", left_index = True, right_index = True), join_list)\n",
    "    \n",
    "    # encoded pheno\n",
    "    omics = pd.merge(left=pheno, right=omics, how=\"inner\", left_index=True, right_index=True)\n",
    "    \n",
    "    # list to dict\n",
    "    omics_label = [omics, rna, mirna, mt]\n",
    "    data_type = [\"omics\", \"rna\", \"mirna\", \"mt\"]\n",
    "    omics_group = list(map(lambda df : pd.merge(left=group, right=df, how=\"inner\", \n",
    "                                          left_index=True, right_index=True), omics_label))\n",
    "    zipbObj = zip(data_type, omics_group)\n",
    "    omics = dict(zipbObj)\n",
    "    \n",
    "    return omics\n",
    "\n",
    "def select_top_n(X, y, cv, method_, univariate):    \n",
    "    # Pipeline\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    \n",
    "    if univariate:\n",
    "        # grid search\n",
    "        fs = SelectKBest(score_func=method_)\n",
    "        pipeline = Pipeline(steps=[('method',fs), ('lr', model)])\n",
    "        grid = dict()\n",
    "        \n",
    "        if len(X.columns) < 1000 :\n",
    "            grid['method__k'] = list(range(100, len(X.columns), 100))\n",
    "        else:\n",
    "            grid['method__k'] = list(range(100, 1100, 100))\n",
    "    else :\n",
    "        # grid search\n",
    "        fs = SelectFromModel(estimator=method_)\n",
    "        pipeline = Pipeline(steps=[('method',fs), ('lr', model)])\n",
    "        grid = dict()\n",
    "        \n",
    "        if len(X.columns) < 1000 :\n",
    "            grid['method__max_features'] = list(range(100, len(X.columns), 100))\n",
    "        else:\n",
    "            grid['method__max_features'] = list(range(100, 1100, 100))\n",
    "\n",
    "    # define the grid search\n",
    "    search = GridSearchCV(pipeline, grid, scoring='f1_micro', n_jobs=-1, cv=cv)\n",
    "    results = search.fit(X, y)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def select_features(X, y, N, method):   \n",
    "    fs = SelectKBest(score_func=method, k= N)\n",
    "    fs.fit(X, y)\n",
    "    \n",
    "    # transform train input data\n",
    "    X_fs = fs.transform(X)\n",
    "    \n",
    "    return X_fs, fs\n",
    "\n",
    "def select_features_ml(X, y, N, method):   \n",
    "    fs = SelectFromModel(estimator=method, max_features=N)\n",
    "    fs.fit(X, y)\n",
    "    \n",
    "    # transform train input data\n",
    "    X_fs = fs.transform(X)\n",
    "    \n",
    "    return X_fs, fs\n",
    "\n",
    "def Feature_selection(group, feature, method, univariate):\n",
    "    '''\n",
    "        @group - target variable\n",
    "        @feautre - feature\n",
    "        @method - annovar -> f_classif / mutual -> mutual_info_classif / rf -> RandomForest / xg -> XGboost / \n",
    "    '''\n",
    "    if univariate:\n",
    "        if method == \"anova\":\n",
    "            method_ = f_classif\n",
    "        else : # mutal\n",
    "            method_ = mutual_info_classif\n",
    "\n",
    "        # Select Top - N using cv(cross-validation)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2, random_state=331)\n",
    "\n",
    "        # grid search\n",
    "        gird_result = select_top_n(feature, group, cv, method_, univariate=True)\n",
    "\n",
    "        # Best Top-K\n",
    "        N = gird_result.best_params_['method__k']\n",
    "        B = gird_result.best_score_*100\n",
    "\n",
    "        #Select N feature\n",
    "        #feature selection : f_classif, mutual_info_classif\n",
    "        feature_fs, fs = select_features(feature, group, N, method_)\n",
    "\n",
    "        # result DF\n",
    "        result_df = pd.DataFrame(fs.scores_, columns=[method])\n",
    "        result_df['feature'] = fs.feature_names_in_\n",
    "        result_df = result_df.sort_values(by = [method], axis=0, ascending=False).iloc[1:N,:]\n",
    "\n",
    "        return result_df.reset_index(drop=True), N, B\n",
    "    \n",
    "    else :\n",
    "        if method == \"rf\":\n",
    "            if len(feature.columns) > 10000:\n",
    "                method_ = RandomForestClassifier(n_estimators = 500)\n",
    "            else:\n",
    "                method_ = RandomForestClassifier()\n",
    "        else : \n",
    "            method_ = XGBClassifier()\n",
    "\n",
    "        # Select Top - N using cv(cross-validation)\n",
    "        cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=331)\n",
    "\n",
    "        # grid search\n",
    "        gird_result = select_top_n(feature, group, cv, method_, univariate=False)\n",
    "        \n",
    "        # Best Top-K\n",
    "        N = gird_result.best_params_['method__max_features']\n",
    "        B = gird_result.best_score_*100\n",
    "\n",
    "        # Select N feature\n",
    "        feature_fs, fs = select_features_ml(feature, group, N, method_)\n",
    "\n",
    "        # result DF\n",
    "        result_df = pd.DataFrame(fs.get_feature_names_out(), columns=[method])\n",
    "\n",
    "        # feature DF, ACC, \n",
    "        return result_df.reset_index(drop=True), N, B\n",
    "    \n",
    "def feature_selection_svm(data_type, o):\n",
    "    feature_result = dict()   \n",
    "    for d_type in data_type:           \n",
    "        anova = Feature_selection(group=o[d_type].loc[:, \"group\"], \n",
    "                                  feature=o[d_type].loc[:, o[d_type].columns != \"group\"],\n",
    "                                  method=\"anova\", univariate=True)\n",
    "        \n",
    "        rf = Feature_selection(group=o[d_type].loc[:, \"group\"], \n",
    "                               feature=o[d_type].loc[:, o[d_type].columns != \"group\"],\n",
    "                               method=\"rf\", univariate=False)\n",
    "\n",
    "        feature_result[d_type] = [anova, rf]\n",
    "        \n",
    "    return feature_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250f18a-b0e2-41cb-adcf-747d65735643",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309ba2a-6ff8-4ede-b512-1b693d26a632",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **File Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cf692-b373-4006-b1cb-3eeaf0326f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c136126-7d9c-4902-9e12-210a680b3ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CANCER_TYPE = \"LIHC\"\n",
    "RAW_file_path = os.getcwd() + \"/RAW_DATA/\"\n",
    "PKL_file_path = os.getcwd() + \"/pkl/\"\n",
    "MODEL_PATH = os.getcwd() + \"/models/\"\n",
    "TENSORBOARD_PATH = os.getcwd() + '/log'\n",
    "GROUP_PHTH = os.getcwd() + '/group/'\n",
    "PNG_PATH = os.getcwd() + '/png/'\n",
    "GROUP_VALIDATION_PATH = os.getcwd() + '/group_validation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24e8ea-7352-466c-b386-1d9660d7a535",
   "metadata": {},
   "source": [
    "* **Data-Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057f640-c1d9-4e99-8a17-7f226ec7e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = load_tcga_dataset(pkl_path=PKL_file_path, raw_path=RAW_file_path, cancer_type=CANCER_TYPE, norm=True)\n",
    "X_train, X_test = train_test_split(omics, test_size = .2, random_state = 21, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88473f47-ded2-482e-92d4-4fefaccd663f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Best Auto-Encoder & K-Mean Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75030a-7643-4c2d-859d-97c78c5b6861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _ in range(5000):\n",
    "    log_pvalue_l, silhouette_score_l, rna_anovar_f1, rna_rf_f1 = [], [], [], []\n",
    "    mirna_anovar_f1, mirna_rf_f1, mt_anovar_f1, mt_rf_f1 = [], [], [], []\n",
    "    file_name = []\n",
    "    \n",
    "    FILE_NAME = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    file_name.append(FILE_NAME)\n",
    "    print(FILE_NAME)\n",
    "    \n",
    "    ## AE(vanilla, sparse, denoisy) - Model compile & Fit\n",
    "    encoder_vanilla = run_ae(X_train=X_train, X_test=X_test, tensorboard_path=TENSORBOARD_PATH)\n",
    "    encoder_sparse = run_ae_sparse(X_train=X_train, X_test=X_test, tensorboard_path=TENSORBOARD_PATH)\n",
    "    encoder_denoisy = run_ae_denoisy(X_train=X_train, X_test=X_test, tensorboard_path=TENSORBOARD_PATH)\n",
    "\n",
    "    group, silhouette_score = best_ae_model(model_list=[encoder_vanilla, encoder_sparse, encoder_denoisy], o=omics,\n",
    "                  group_path=GROUP_PHTH, model_path=MODEL_PATH, cancer_type=CANCER_TYPE, file_name=FILE_NAME)\n",
    "\n",
    "    ## Sub-group Evalutation\n",
    "    ### load preprocess data\n",
    "    omics_preprocess = load_preprocess_tcga_dataset(pkl_path=PKL_file_path, raw_path=RAW_file_path, group=group, norm=True, \n",
    "                                                   cancer_type=\"LIHC\")                                         \n",
    "\n",
    "    ### Feature Selection(Anova, RandomForest) for SVM\n",
    "    feature_result = feature_selection_svm(data_type=[\"rna\", \"mirna\", \"mt\"], o=omics_preprocess)\n",
    "\n",
    "    ### Survival Analysis - logranktest\n",
    "    log_pvalue = log_rank_test(df=omics_preprocess[\"omics\"], png_path=PNG_PATH, file_name=FILE_NAME)\n",
    "\n",
    "    ### Score\n",
    "    log_pvalue_l.append(log_pvalue)\n",
    "    silhouette_score_l.append(silhouette_score)\n",
    "\n",
    "    rna_anovar_f1.append(feature_result[\"rna\"][0][2])\n",
    "    rna_rf_f1.append(feature_result[\"rna\"][1][2])\n",
    "\n",
    "    mirna_anovar_f1.append(feature_result[\"mirna\"][0][2])\n",
    "    mirna_rf_f1.append(feature_result[\"mirna\"][1][2])\n",
    "\n",
    "    mt_anovar_f1.append(feature_result[\"mt\"][0][2])\n",
    "    mt_rf_f1.append(feature_result[\"mt\"][1][2])\n",
    "    \n",
    "    # Write Score DF\n",
    "    score_df = pd.DataFrame({\n",
    "    'FILENAME' : file_name,\n",
    "    'Log Rank Test' : log_pvalue_l,\n",
    "    'Silhouette' : silhouette_score_l,\n",
    "    'RNA_ANOVA_F1' : rna_anovar_f1,\n",
    "    'RNA_RF_F1' : rna_rf_f1,\n",
    "    'miRNA_ANOVA_F1' : mirna_anovar_f1,\n",
    "    'miRNA_RF_F1' : mirna_rf_f1,\n",
    "    'Methylation_ANOVA_F1' : mt_anovar_f1,\n",
    "    'Methylation_RF_F1' : mt_rf_f1\n",
    "    })\n",
    "\n",
    "    # file check\n",
    "    if not os.path.exists(GROUP_VALIDATION_PATH + \"test_validation.csv\"):\n",
    "        score_df.to_csv(GROUP_VALIDATION_PATH + \"test_validation.csv\", index=False, mode='w')\n",
    "    else:\n",
    "        score_df.to_csv(GROUP_VALIDATION_PATH + \"test_validation.csv\", index=False, mode='a', header=False)\n",
    "        \n",
    "    # session clear\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e177d9-9fbe-45e5-941e-dfb74d65fa58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomic",
   "language": "python",
   "name": "multiomics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
