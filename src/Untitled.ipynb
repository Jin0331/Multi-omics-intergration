{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancer_select(DF, cancer_type):\n",
    "    # phenotype\n",
    "    phe1 = pd.read_csv(\"https://gdc-hub.s3.us-east-1.amazonaws.com/download/GDC-PANCAN.basic_phenotype.tsv.gz\", sep=\"\\t\")\n",
    "    phe1 = phe1.loc[phe1.program == \"TCGA\", :].loc[:, ['sample', 'sample_type', 'project_id']].drop_duplicates(['sample'])\n",
    "    phe1['sample'] =  phe1.apply(lambda x : x['sample'][:-1], axis=1)\n",
    "    phe2 = pd.read_csv(\"https://tcga-pancan-atlas-hub.s3.us-east-1.amazonaws.com/download/TCGA_phenotype_denseDataOnlyDownload.tsv.gz\", sep=\"\\t\")\n",
    "    ph_join = pd.merge(left = phe2 , right = phe1, how = \"left\", on = \"sample\").dropna(subset=['project_id'])\n",
    "    \n",
    "    if cancer_type == \"PAN\" or cancer_type == \"PANCAN\":\n",
    "        filterd = ph_join.loc[ph_join['sample_type_y'] == \"Primary Tumor\", :]\n",
    "        sample_barcode = filterd[\"sample\"].tolist()\n",
    "    else:\n",
    "        filterd = ph_join.loc[(ph_join['sample_type_y'] == \"Primary Tumor\") & (ph_join['project_id'] == \"TCGA-\" + cancer_type) , :]\n",
    "        sample_barcode = filterd[\"sample\"].tolist()\n",
    "        \n",
    "    intersect_ = list(set(DF.columns.tolist()).intersection(sample_barcode))\n",
    "    \n",
    "    return intersect_\n",
    "\n",
    "def non_zero_column(DF):\n",
    "    sample_cnt = int(len(DF.columns) * 0.2)\n",
    "    zero_row = dict(DF.isin([0]).sum(axis=1))\n",
    "    non_remove_feature = list()\n",
    "\n",
    "    for key, value in zero_row.items():\n",
    "        if value < sample_cnt:\n",
    "            non_remove_feature.append(key)\n",
    "    \n",
    "    return non_remove_feature\n",
    "\n",
    "\n",
    "def determine_outlier_thresholds_iqr(dataframe, col_name, th1=0.05, th3=0.95):\n",
    "    quartile1 = dataframe[col_name].quantile(th1)\n",
    "    quartile3 = dataframe[col_name].quantile(th3)\n",
    "    iqr = quartile3 - quartile1\n",
    "    upper_limit = quartile3 + 1.5 * iqr\n",
    "    lower_limit = quartile1 - 1.5 * iqr\n",
    "    return lower_limit, upper_limit\n",
    "\n",
    "def check_outliers_iqr(dataframe, col_name):\n",
    "    lower_limit, upper_limit = determine_outlier_thresholds_iqr(dataframe, col_name)\n",
    "    if dataframe[(dataframe[col_name] > upper_limit) | (dataframe[col_name] < lower_limit)].any(axis=None):\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def replace_with_thresholds_iqr(dataframe, cols, th1=0.05, th3=0.95, replace=False):\n",
    "    from tabulate import tabulate\n",
    "    data = []\n",
    "    for col_name in cols:\n",
    "        if col_name != 'Outcome':\n",
    "            outliers_ = check_outliers_iqr(dataframe,col_name)\n",
    "            count = None\n",
    "            lower_limit, upper_limit = determine_outlier_thresholds_iqr(dataframe, col_name, th1, th3)\n",
    "            if outliers_:\n",
    "                count = dataframe[(dataframe[col_name] > upper_limit) | (dataframe[col_name] < lower_limit)][col_name].count()\n",
    "                if replace: \n",
    "                    dataframe.loc[(dataframe[col_name] < lower_limit), col_name] = lower_limit\n",
    "                    dataframe.loc[(dataframe[col_name] > upper_limit), col_name] = upper_limit\n",
    "            outliers_status = check_outliers_iqr(dataframe, col_name)\n",
    "            data.append([outliers_, outliers_status, count, col_name, lower_limit, upper_limit ])\n",
    "    table = tabulate(data, headers=['Outliers (Previously)', 'Outliers', 'Count', 'Column', 'Lower Limit', 'Upper Limit'], tablefmt='rst', numalign='right')\n",
    "   \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def get_dataset(path, cancer_type, std):\n",
    "    # non_scale dataset\n",
    "    # load raw DF\n",
    "    DF = pd.read_csv(path + \"/feature/input_feature_non_scale/\" + cancer_type + \".txt\", sep = \"\\t\")\n",
    "    pr_sample_DF = DF.iloc[:, 0:1]\n",
    "\n",
    "    if cancer_type == \"PANCAN\":\n",
    "        DF = DF.drop([\"Sample_barcode\"], axis=1)\n",
    "    else :\n",
    "        DF = DF.drop([\"Project\", \"Sample_barcode\"], axis=1)\n",
    "        \n",
    "    # remove outlier\n",
    "    DF = replace_with_thresholds_iqr(dataframe=DF, cols=DF.columns)\n",
    "    max_number_of_nas = DF.shape[0] / 2\n",
    "    DF = DF.loc[:, (DF.isnull().sum(axis=0) <= max_number_of_nas)]\n",
    "\n",
    "    # normalization\n",
    "    if std:\n",
    "        scalerX = StandardScaler()\n",
    "        scalerX.fit(DF)\n",
    "        DF_sacle = scalerX.transform(DF)\n",
    "    else :\n",
    "        scalerX = MinMaxScaler()\n",
    "        scalerX.fit(X_train)\n",
    "        DF_sacle = scalerX.transform(DF)\n",
    "    \n",
    "    # missing impute\n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    DF_sacle_impute = imputer.fit_transform(DF_sacle)\n",
    "\n",
    "    DF_sacle_impute = pd.DataFrame(DF_sacle_impute, columns=DF.columns)\n",
    "\n",
    "    return DF_sacle_impute\n",
    "\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def get_train_test_DF(x, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, shuffle=True, random_state=32, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def createFolder(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print ('Folder already exists. ' +  path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **File Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_file_path = \"/home/wmbio/WORK/multi-omics/HCC_integration/RAW_DATA/\"\n",
    "MODEL_path = \"/home/jovyan/work/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **RNA gene expression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = pd.read_csv(RAW_file_path + \"tcga_RSEM_Hugo_norm_count.gz\", sep = \"\\t\", index_col=0)\n",
    "rna = rna[cancer_select(DF=rna,cancer_type=\"LIHC\")]\n",
    "rna = rna[rna.index.isin(non_zero_column(rna))].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **miRNA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirna = pd.read_csv(RAW_file_path + \"pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena.gz\", sep = \"\\t\", index_col=0)\n",
    "mirna = mirna[cancer_select(DF=mirna,cancer_type=\"LIHC\")]\n",
    "mirna = mirna[mirna.index.isin(non_zero_column(mirna))].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Methylation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Methylation -> duplicate gene ->>>> average value\n",
    "    mt_map <- fread(file = paste0(RAW_file_path, \"probeMap_illuminaMethyl450_hg19_GPL16304_TCGAlegacy\"), sep = \"\\t\")  \n",
    "    \n",
    "    mt <- fread(paste0(RAW_file_path, \"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena.gz\"), check.names = T) %>%\n",
    "      select(sample, any_of(cnacer_selector(\"LIHC\"))) %>%\n",
    "      left_join(x = ., y = mt_map, by = c(\"sample\" = \"#id\" )) %>% \n",
    "      select(-sample, -strand) %>% \n",
    "      select(gene, chrom, chromStart, chromEnd, everything()) %>% \n",
    "      filter(gene != \".\", !is.na(chrom)) %>% \n",
    "      arrange(chrom, gene) %>%\n",
    "      select(-chrom, -chromStart, -chromEnd) %>%\n",
    "      group_by(gene) %>% \n",
    "      summarise_if(is.numeric, mean, na.rm = TRUE) %>%\n",
    "      transpose_df() %>%\n",
    "      select(-any_of(zero_col_remove(.))) %>%\n",
    "      rename(sample = gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_map = pd.read_csv(RAW_file_path + \"probeMap_illuminaMethyl450_hg19_GPL16304_TCGAlegacy\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/multiomics/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_751819/3041148772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_file_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/multiomics/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multiomics/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multiomics/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multiomics/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/multiomics/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mt = pd.read_csv(RAW_file_path + \"jhu-usc.edu_PANCAN_HumanMethylation450.betaValue_whitelisted.tsv.synapse_download_5096262.xena.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiomic_37",
   "language": "python",
   "name": "multiomics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
